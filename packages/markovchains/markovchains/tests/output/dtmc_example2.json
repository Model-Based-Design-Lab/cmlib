{
    "states": "[A, B, C, D]",
    "rewardVector": "[3.0000, 5.0000, 8.0000, 2.0000]",
    "executeSteps_0": "[[1.0000, 0.0000, 0.0000, 0.0000]]",
    "executeSteps_15": "[[1.0000, 0.0000, 0.0000, 0.0000], [0.0000, 1.0000, 0.0000, 0.0000], [0.0000, 0.0000, 1.0000, 0.0000], [0.0000, 0.8333, 0.0000, 0.1667], [0.1667, 0.0000, 0.8333, 0.0000], [0.0000, 0.8611, 0.0000, 0.1389], [0.1389, 0.0000, 0.8611, 0.0000], [0.0000, 0.8565, 0.0000, 0.1435], [0.1435, 0.0000, 0.8565, 0.0000], [0.0000, 0.8573, 0.0000, 0.1427], [0.1427, 0.0000, 0.8573, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429], [0.1429, 0.0000, 0.8571, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429], [0.1429, 0.0000, 0.8571, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429]]",
    "classifyTransientRecurrent": "[, [A, B, C, D]]",
    "classifyPeriodicity": {
        "C": "2.0000",
        "A": "2.0000",
        "D": "2.0000",
        "B": "2.0000"
    },
    "determineMCType": "non-ergodic unichain",
    "hittingProbabilities": {
        "A": "1.0000",
        "B": "1.0000",
        "C": "1.0000",
        "D": "1.0000"
    },
    "limitingMatrix": "[[0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714]]",
    "limitingDistribution": "[0.0714, 0.4286, 0.4286, 0.0714]",
    "longRunReward": "5.9286",
    "transient_reward_0_step_0": "3.0000",
    "transient_reward_10_step_0": "3.0000",
    "transient_reward_10_step_1": "5.0000",
    "transient_reward_10_step_2": "8.0000",
    "transient_reward_10_step_3": "4.5000",
    "transient_reward_10_step_4": "7.1667",
    "transient_reward_10_step_5": "4.5833",
    "transient_reward_10_step_6": "7.3056",
    "transient_reward_10_step_7": "4.5694",
    "transient_reward_10_step_8": "7.2824",
    "transient_reward_10_step_9": "4.5718",
    "transient_reward_10_step_10": "7.2863",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, B, C, B, C, B, C, B, C, B, C, B, C, B, C, D]",
    "longRunExpectedAverageReward_cycle": "[[4.5000, 4.5000], 0.0000, 0.0000, 4.5000, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[5.2062, 6.1938], 0.4938, 0.0949, 5.7000, 5.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[5.2504, 5.5678], 0.1587, 0.0302, 5.4091, 3.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [[-0.1633, 0.1633], [-0.8167, 0.8167], [-0.8167, 0.8167], [-0.1633, 0.1633]], [0.1633, 0.8167, 0.8167, 0.1633], [None, None, None, None], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], [None, None, None, None], 0.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[None, [[-0.0612, 0.0612], [-0.4881, 0.4881], [-0.4881, 0.4881], [-0.0612, 0.0612]], [0.0612, 0.4881, 0.4881, 0.0612], [None, None, None, None], 4.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[None, [[-0.0154, 0.0154], [-0.1725, 0.1725], [-0.1725, 0.1725], [-0.0154, 0.0154]], [0.0154, 0.1725, 0.1725, 0.0154], [None, None, None, None], 61.0000, Maximum path length]",
    "estimationExpectedReward_step": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 1.0000, 0.0000, 0.0000], [[0.0000, 0.0000], [1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, 0.0000], [None, 1.0000, 1.0000, 0.0000], [[0.0000, 0.0000], None, None, [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, 0.0000], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [None, None, None, 0.0000], [1.0000, 1.0000, 1.0000, 0.0000], [None, None, None, [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[0.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [None, None, None, 0.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]"
}