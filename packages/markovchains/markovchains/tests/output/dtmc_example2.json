{
    "states": "[A, B, C, D]",
    "rewardVector": "[3.0000, 5.0000, 8.0000, 2.0000]",
    "executeSteps_0": "[[1.0000, 0.0000, 0.0000, 0.0000]]",
    "executeSteps_15": "[[1.0000, 0.0000, 0.0000, 0.0000], [0.0000, 1.0000, 0.0000, 0.0000], [0.0000, 0.0000, 1.0000, 0.0000], [0.0000, 0.8333, 0.0000, 0.1667], [0.1667, 0.0000, 0.8333, 0.0000], [0.0000, 0.8611, 0.0000, 0.1389], [0.1389, 0.0000, 0.8611, 0.0000], [0.0000, 0.8565, 0.0000, 0.1435], [0.1435, 0.0000, 0.8565, 0.0000], [0.0000, 0.8573, 0.0000, 0.1427], [0.1427, 0.0000, 0.8573, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429], [0.1429, 0.0000, 0.8571, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429], [0.1429, 0.0000, 0.8571, 0.0000], [0.0000, 0.8571, 0.0000, 0.1429]]",
    "classifyTransientRecurrent": "[, [A, B, C, D]]",
    "classifyPeriodicity": {
        "B": "2.0000",
        "C": "2.0000",
        "A": "2.0000",
        "D": "2.0000"
    },
    "determineMCType": "non-ergodic unichain",
    "hittingProbabilities": {
        "A": "1.0000",
        "B": "1.0000",
        "C": "1.0000",
        "D": "1.0000"
    },
    "limitingMatrix": "[[0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714], [0.0714, 0.4286, 0.4286, 0.0714]]",
    "limitingDistribution": "[0.0714, 0.4286, 0.4286, 0.0714]",
    "longRunReward": "5.9286",
    "transient_reward_0_step_0": "3.0000",
    "transient_reward_10_step_0": "3.0000",
    "transient_reward_10_step_1": "5.0000",
    "transient_reward_10_step_2": "8.0000",
    "transient_reward_10_step_3": "4.5000",
    "transient_reward_10_step_4": "7.1667",
    "transient_reward_10_step_5": "4.5833",
    "transient_reward_10_step_6": "7.3056",
    "transient_reward_10_step_7": "4.5694",
    "transient_reward_10_step_8": "7.2824",
    "transient_reward_10_step_9": "4.5718",
    "transient_reward_10_step_10": "7.2863",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, B, C, B, C, B, C, B, C, D, A, B, C, B, C, D]",
    "longRunExpectedAverageReward_cycle": "[[5.1667, 5.1667], 0.0000, 0.0000, 5.1667, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[[None, None], None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[5.2568, 6.2047], 0.4739, 0.0902, 5.7308, 5.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[5.1535, 5.8465], 0.3465, 0.0672, 5.5000, 2.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[[0.0714, 0.4286, 0.4286, 0.0714], [[0.0714, 0.0714], [0.4286, 0.4286], [0.4286, 0.4286], [0.0714, 0.0714]], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[[0.0000, 0.0000, 0.0000, 0.0000], [[None, None], [None, None], [None, None], [None, None]], [None, None, None, None], [None, None, None, None], 0.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[0.0714, 0.4286, 0.4286, 0.0714], [[0.0149, 0.1280], [0.3720, 0.4851], [0.3720, 0.4851], [0.0149, 0.1280]], [0.0566, 0.0566, 0.0566, 0.0566], [3.8064, 0.1521, 0.1521, 3.8064], 2.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[[0.0802, 0.4198, 0.4198, 0.0802], [[0.0542, 0.1063], [0.3937, 0.4458], [0.3937, 0.4458], [0.0542, 0.1063]], [0.0261, 0.0261, 0.0261, 0.0261], [0.4813, 0.0662, 0.0662, 0.4813], 13.0000, Relative Error]",
    "estimationExpectedReward_step": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 1.0000, 0.0000, 0.0000], [[0.0000, 0.0000], [1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, 0.0000], [None, 1.0000, 1.0000, 0.0000], [[0.0000, 0.0000], [None, None], [None, None], [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, 0.0000], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [None, None, None, 0.0000], [1.0000, 1.0000, 1.0000, 0.0000], [[None, None], [None, None], [None, None], [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[0.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [None, None, None, 0.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]"
}