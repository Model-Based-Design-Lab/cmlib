{
    "states": "[A, B, C, D]",
    "rewardVector": "[3, 5, 8, 2]",
    "executeSteps_0": "[[1, 0, 0, 0]]",
    "executeSteps_15": "[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 5/6, 0, 1/6], [1/6, 0, 5/6, 0], [0, 31/36, 0, 5/36], [5/36, 0, 31/36, 0], [0, 185/216, 0, 31/216], [31/216, 0, 185/216, 0], [0, 1111/1296, 0, 185/1296], [185/1296, 0, 1111/1296, 0], [0, 6665/7776, 0, 1111/7776], [1111/7776, 0, 6665/7776, 0], [0, 39991/46656, 0, 6665/46656], [6665/46656, 0, 39991/46656, 0], [0, 239945/279936, 0, 39991/279936]]",
    "classifyTransientRecurrent": "[, [A, B, C, D]]",
    "classifyPeriodicity": {
        "B": "2.0000",
        "D": "2.0000",
        "A": "2.0000",
        "C": "2.0000"
    },
    "determineMCType": "non-ergodic unichain",
    "hittingProbabilities": {
        "A": "1",
        "B": "1",
        "C": "1",
        "D": "1"
    },
    "limitingMatrix": "[[1/14, 1/14, 1/14, 1/14], [3/7, 3/7, 3/7, 3/7], [3/7, 3/7, 3/7, 3/7], [1/14, 1/14, 1/14, 1/14]]",
    "limitingDistribution": "[1/14, 3/7, 3/7, 1/14]",
    "longRunReward": "83/14",
    "transient_reward_0_step_0": "3",
    "transient_reward_10_step_0": "3",
    "transient_reward_10_step_1": "5",
    "transient_reward_10_step_2": "8",
    "transient_reward_10_step_3": "9/2",
    "transient_reward_10_step_4": "43/6",
    "transient_reward_10_step_5": "55/12",
    "transient_reward_10_step_6": "263/36",
    "transient_reward_10_step_7": "329/72",
    "transient_reward_10_step_8": "1573/216",
    "transient_reward_10_step_9": "1975/432",
    "transient_reward_10_step_10": "9443/1296",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, B, C, B, C, B, C, B, C, B, C, B, C, B, C, D]",
    "longRunExpectedAverageReward_cycle": "[None, None, None, None, None, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, None, None, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[5.6035, 5.9419], 0.1692, 0.0302, 5.7727, 5.2006, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[5.7936, 6.2308], 0.2186, 0.0377, 6.0122, 10.0205, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], None, None, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[[0.0490, 0.0867], [0.4133, 0.4510], [0.4133, 0.4510], [0.0490, 0.0867]], [0.0188, 0.0188, 0.0188, 0.0188], [0.3841, 0.0456, 0.0456, 0.3841], [0.0679, 0.4321, 0.4321, 0.0679], [0.7754, 0.7754, 0.7754, 0.7754], Absolute Error]",
    "cezaroLimitDistribution_rel": "[[[0.0674, 0.1155], [0.3845, 0.4326], [0.3845, 0.4326], [0.0674, 0.1155]], [0.0241, 0.0241, 0.0241, 0.0241], [0.3574, 0.0626, 0.0626, 0.3574], [0.0915, 0.4085, 0.4085, 0.0915], [0.7359, 0.7359, 0.7359, 0.7359], Relative Error]",
    "estimationExpectedReward_step": "[None, None, None, None, None, Number of Paths]",
    "estimationExpectedReward_abs": "[[5.0000, 5.0000], 0.0000, 0.0000, 5.0000, 0.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[[5.0000, 5.0000], 0.0000, 0.0000, 5.0000, 0.0000, Relative Error]",
    "estimationDistribution": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of Paths]",
    "estimationHittingState": "[{'A': '[None, None, None, None, None]', 'B': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]', 'D': '[None, None, None, None, None]'}, {'A': 'Number of Paths', 'B': 'Number of Paths', 'C': 'Number of Paths', 'D': 'Number of Paths'}]",
    "estimationHittingreward": "None",
    "estimationHittingStateSet": "[{'A': '[None, None, None, None, None]', 'B': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]', 'D': '[None, None, None, None, None]'}, {'A': 'Number of Paths', 'B': 'Number of Paths', 'C': 'Number of Paths', 'D': 'Number of Paths'}]",
    "estimationHittingRewardSet": "None"
}