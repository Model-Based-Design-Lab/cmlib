{
    "states": "[A, B, C, D]",
    "rewardVector": "[3, 5, 8, 2]",
    "executeSteps_0": "[[1, 0, 0, 0]]",
    "executeSteps_15": "[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 5/6, 0, 1/6], [1/6, 0, 5/6, 0], [0, 31/36, 0, 5/36], [5/36, 0, 31/36, 0], [0, 185/216, 0, 31/216], [31/216, 0, 185/216, 0], [0, 1111/1296, 0, 185/1296], [185/1296, 0, 1111/1296, 0], [0, 6665/7776, 0, 1111/7776], [1111/7776, 0, 6665/7776, 0], [0, 39991/46656, 0, 6665/46656], [6665/46656, 0, 39991/46656, 0], [0, 239945/279936, 0, 39991/279936]]",
    "classifyTransientRecurrent": "[, [A, B, C, D]]",
    "classifyPeriodicity": {
        "C": "2.0000",
        "B": "2.0000",
        "D": "2.0000",
        "A": "2.0000"
    },
    "determineMCType": "non-ergodic unichain",
    "hittingProbabilities": {
        "A": "1",
        "B": "1",
        "C": "1",
        "D": "1"
    },
    "limitingMatrix": "[[1/14, 1/14, 1/14, 1/14], [3/7, 3/7, 3/7, 3/7], [3/7, 3/7, 3/7, 3/7], [1/14, 1/14, 1/14, 1/14]]",
    "limitingDistribution": "[1/14, 3/7, 3/7, 1/14]",
    "longRunReward": "83/14",
    "transient_reward_0_step_0": "3",
    "transient_reward_10_step_0": "3",
    "transient_reward_10_step_1": "5",
    "transient_reward_10_step_2": "8",
    "transient_reward_10_step_3": "9/2",
    "transient_reward_10_step_4": "43/6",
    "transient_reward_10_step_5": "55/12",
    "transient_reward_10_step_6": "263/36",
    "transient_reward_10_step_7": "329/72",
    "transient_reward_10_step_8": "1573/216",
    "transient_reward_10_step_9": "1975/432",
    "transient_reward_10_step_10": "9443/1296",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, B, C, B, C, B, C, B, C, B, C, B, C, B, C, D]",
    "longRunExpectedAverageReward_cycle": "[None, None, None, None, None, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, None, None, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[5.5852, 5.9383], 0.1766, 0.0316, 5.7618, 5.2470, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[5.9224, 6.1337], 0.1056, 0.0178, 6.0281, 4.9097, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], None, None, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[[-0.0226, 0.1559], [0.2881, 0.6452], [0.2560, 0.6107], [-0.0309, 0.0976]], [0.0893, 0.1785, 0.1773, 0.0642], [None, 0.6196, 0.6926, None], [0.0667, 0.4667, 0.4333, 0.0333], [0.2494, 0.4989, 0.4955, 0.1795], Absolute Error]",
    "cezaroLimitDistribution_rel": "[[[0.0461, 0.0914], [0.3869, 0.4756], [0.3869, 0.4756], [0.0461, 0.0914]], [0.0226, 0.0443, 0.0443, 0.0226], [0.4909, 0.1145, 0.1145, 0.4909], [0.0688, 0.4313, 0.4313, 0.0688], [0.2530, 0.4953, 0.4953, 0.2530], Relative Error]",
    "estimationExpectedReward_step": "[None, None, None, None, None, Number of Paths]",
    "estimationExpectedReward_abs": "[[5.0000, 5.0000], 0.0000, 0.0000, 5.0000, 0.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[[5.0000, 5.0000], 0.0000, 0.0000, 5.0000, 0.0000, Relative Error]",
    "estimationDistribution": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of Paths]",
    "estimationHittingState": "[{'A': '[None, None, None, None, None]', 'B': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]', 'D': '[None, None, None, None, None]'}, {'A': 'Number of Paths', 'B': 'Number of Paths', 'C': 'Number of Paths', 'D': 'Number of Paths'}]",
    "estimationHittingreward": "None",
    "estimationHittingStateSet": "[{'A': '[None, None, None, None, None]', 'B': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]', 'D': '[None, None, None, None, None]'}, {'A': 'Number of Paths', 'B': 'Number of Paths', 'C': 'Number of Paths', 'D': 'Number of Paths'}]",
    "estimationHittingRewardSet": "None"
}