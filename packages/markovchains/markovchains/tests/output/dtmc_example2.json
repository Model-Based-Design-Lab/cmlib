{
    "states": "[A, B, C, D]",
    "rewardVector": "[3, 5, 8, 2]",
    "executeSteps_0": "[[1, 0, 0, 0]]",
    "executeSteps_15": "[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 5/6, 0, 1/6], [1/6, 0, 5/6, 0], [0, 31/36, 0, 5/36], [5/36, 0, 31/36, 0], [0, 185/216, 0, 31/216], [31/216, 0, 185/216, 0], [0, 1111/1296, 0, 185/1296], [185/1296, 0, 1111/1296, 0], [0, 6665/7776, 0, 1111/7776], [1111/7776, 0, 6665/7776, 0], [0, 39991/46656, 0, 6665/46656], [6665/46656, 0, 39991/46656, 0], [0, 239945/279936, 0, 39991/279936]]",
    "classifyTransientRecurrent": "[, [A, B, C, D]]",
    "classifyPeriodicity": {
        "D": "2.0000",
        "B": "2.0000",
        "A": "2.0000",
        "C": "2.0000"
    },
    "determineMCType": "non-ergodic unichain",
    "hittingProbabilities": {
        "A": "1",
        "B": "1",
        "C": "1",
        "D": "1"
    },
    "limitingMatrix": "[[1/14, 1/14, 1/14, 1/14], [3/7, 3/7, 3/7, 3/7], [3/7, 3/7, 3/7, 3/7], [1/14, 1/14, 1/14, 1/14]]",
    "limitingDistribution": "[1/14, 3/7, 3/7, 1/14]",
    "longRunReward": "83/14",
    "transient_reward_0_step_0": "3",
    "transient_reward_10_step_0": "3",
    "transient_reward_10_step_1": "5",
    "transient_reward_10_step_2": "8",
    "transient_reward_10_step_3": "9/2",
    "transient_reward_10_step_4": "43/6",
    "transient_reward_10_step_5": "55/12",
    "transient_reward_10_step_6": "263/36",
    "transient_reward_10_step_7": "329/72",
    "transient_reward_10_step_8": "1573/216",
    "transient_reward_10_step_9": "1975/432",
    "transient_reward_10_step_10": "9443/1296",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, B, C, B, C, B, C, B, C, B, C, B, C, B, C, D]",
    "longRunExpectedAverageReward_cycle": "[[4.5000, 4.5000], 0.0000, 0.0000, 4.5000, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[5.2062, 6.1938], 0.4938, 0.0949, 5.7000, 5.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[5.2504, 5.5678], 0.1587, 0.0302, 5.4091, 3.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [[-0.1633, 0.1633], [-0.8167, 0.8167], [-0.8167, 0.8167], [-0.1633, 0.1633]], [0.1633, 0.8167, 0.8167, 0.1633], [None, None, None, None], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], [None, None, None, None], 0.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[None, [[-0.0612, 0.0612], [-0.4881, 0.4881], [-0.4881, 0.4881], [-0.0612, 0.0612]], [0.0612, 0.4881, 0.4881, 0.0612], [None, None, None, None], 4.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[None, [[-0.0154, 0.0154], [-0.1725, 0.1725], [-0.1725, 0.1725], [-0.0154, 0.0154]], [0.0154, 0.1725, 0.1725, 0.0154], [None, None, None, None], 61.0000, Maximum path length]",
    "estimationExpectedReward_step": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[5.0000, [5.0000, 5.0000], 0.0000, 0.0000, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 1.0000, 0.0000, 0.0000], [[0.0000, 0.0000], [1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, 0.0000], [None, 1.0000, 1.0000, 0.0000], [[0.0000, 0.0000], None, None, [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, 0.0000], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided, 2.0000], [1.0000, 1.0000, 1.0000, 1.0000], [None, None, None, 0.0000], [1.0000, 1.0000, 1.0000, 0.0000], [None, None, None, [2.0000, 2.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[0.0000, 0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [None, None, None, 0.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]"
}