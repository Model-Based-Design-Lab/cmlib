{
    "states": "[A_s0, B1, C]",
    "rewardVector": "[1, 2, 0]",
    "executeSteps_0": "[[1, 0, 0]]",
    "executeSteps_15": "[[1, 0, 0], [1/2, 1/2, 0], [1/4, 1/2, 1/4], [1/8, 1/2, 3/8], [1/16, 1/2, 7/16], [1/32, 1/2, 15/32], [1/64, 1/2, 31/64], [1/128, 1/2, 63/128], [1/256, 1/2, 127/256], [1/512, 1/2, 255/512], [1/1024, 1/2, 511/1024], [1/2048, 1/2, 1023/2048], [1/4096, 1/2, 2047/4096], [1/8192, 1/2, 4095/8192], [1/16384, 1/2, 8191/16384], [1/32768, 1/2, 16383/32768]]",
    "classifyTransientRecurrent": "[[A_s0], [B1, C]]",
    "classifyPeriodicity": {
        "B1": "1.0000",
        "C": "1.0000"
    },
    "determineMCType": "ergodic unichain",
    "hittingProbabilities": {
        "A_s0": "1",
        "B1": "1",
        "C": "1"
    },
    "limitingMatrix": "[[0, 0, 0], [1/2, 1/2, 1/2], [1/2, 1/2, 1/2]]",
    "limitingDistribution": "[0, 1/2, 1/2]",
    "longRunReward": "1",
    "transient_reward_0_step_0": "1",
    "transient_reward_10_step_0": "1",
    "transient_reward_10_step_1": "3/2",
    "transient_reward_10_step_2": "5/4",
    "transient_reward_10_step_3": "9/8",
    "transient_reward_10_step_4": "17/16",
    "transient_reward_10_step_5": "33/32",
    "transient_reward_10_step_6": "65/64",
    "transient_reward_10_step_7": "129/128",
    "transient_reward_10_step_8": "257/256",
    "transient_reward_10_step_9": "513/512",
    "transient_reward_10_step_10": "1025/1024",
    "markovTrace_0": "[A_s0]",
    "markovTrace_15": "[A_s0, A_s0, B1, B1, C, C, C, B1, C, B1, B1, C, B1, B1, C, B1]",
    "longRunExpectedAverageReward_cycle": "[None, None, None, None, None, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, None, None, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[0.7196, 1.0232], 0.1518, 0.2109, 0.8714, 0.9898, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[0.6990, 1.1779], 0.2395, 0.3426, 0.9385, 1.4500, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [None, None, None], [None, None, None], None, None, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None], [None, None, None], None, None, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[[-0.0226, 0.1559], [0.3211, 0.6789], [0.2560, 0.6107]], [0.0893, 0.1789, 0.1773], [None, 0.5572, 0.6926], [0.0667, 0.5000, 0.4333], [0.2494, 0.5000, 0.4955], Absolute Error]",
    "cezaroLimitDistribution_rel": "[[[0.0001, 0.0079], [0.4925, 0.5544], [0.4416, 0.5035]], [0.0039, 0.0309, 0.0309], [44.5001, 0.0628, 0.0700], [0.0040, 0.5235, 0.4725], [0.0631, 0.4994, 0.4992], Maximum path length]",
    "estimationExpectedReward_step": "[None, None, None, None, None, Number of Paths]",
    "estimationExpectedReward_abs": "[[1.3211, 1.6789], 0.1789, 0.1354, 1.5000, 0.5000, Absolute Error]",
    "estimationExpectedReward_rel": "[[1.4247, 1.7753], 0.1753, 0.1230, 1.6000, 0.4899, Relative Error]",
    "estimationDistribution": "[None, [None, None, None], [None, None, None], None, None, Number of Paths]",
    "estimationHittingState": "[{'A_s0': '[None, None, None, None, None]', 'B1': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]'}, {'A_s0': 'Number of Paths', 'B1': 'Number of Paths', 'C': 'Number of Paths'}]",
    "estimationHittingreward": "[{'A_s0': '[None, None, None, None, None]', 'B1': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]'}, {'A_s0': 'Number of Paths', 'B1': 'Number of Paths', 'C': 'Number of Paths'}]",
    "estimationHittingStateSet": "[{'A_s0': '[None, None, None, None, None]', 'B1': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]'}, {'A_s0': 'Number of Paths', 'B1': 'Number of Paths', 'C': 'Number of Paths'}]",
    "estimationHittingRewardSet": "[{'A_s0': '[None, None, None, None, None]', 'B1': '[None, None, None, None, None]', 'C': '[None, None, None, None, None]'}, {'A_s0': 'Number of Paths', 'B1': 'Number of Paths', 'C': 'Number of Paths'}]"
}