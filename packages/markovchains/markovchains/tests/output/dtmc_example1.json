{
    "states": "[A_s0, B1, C]",
    "rewardVector": "[1.0000, 2.0000, 0.0000]",
    "executeSteps_0": "[[1.0000, 0.0000, 0.0000]]",
    "executeSteps_15": "[[1.0000, 0.0000, 0.0000], [0.5000, 0.5000, 0.0000], [0.2500, 0.5000, 0.2500], [0.1250, 0.5000, 0.3750], [0.0625, 0.5000, 0.4375], [0.0312, 0.5000, 0.4688], [0.0156, 0.5000, 0.4844], [0.0078, 0.5000, 0.4922], [0.0039, 0.5000, 0.4961], [0.0020, 0.5000, 0.4980], [0.0010, 0.5000, 0.4990], [0.0005, 0.5000, 0.4995], [0.0002, 0.5000, 0.4998], [0.0001, 0.5000, 0.4999], [0.0001, 0.5000, 0.4999], [0.0000, 0.5000, 0.5000]]",
    "classifyTransientRecurrent": "[[A_s0], [B1, C]]",
    "classifyPeriodicity": {
        "C": "1.0000",
        "B1": "1.0000"
    },
    "determineMCType": "ergodic unichain",
    "hittingProbabilities": {
        "A_s0": "1.0000",
        "B1": "1.0000",
        "C": "1.0000"
    },
    "limitingMatrix": "[[0.0000, 0.5000, 0.5000], [0.0000, 0.5000, 0.5000], [0.0000, 0.5000, 0.5000]]",
    "limitingDistribution": "[0.0000, 0.5000, 0.5000]",
    "longRunReward": "1.0000",
    "transient_reward_0_step_0": "1.0000",
    "transient_reward_10_step_0": "1.0000",
    "transient_reward_10_step_1": "1.5000",
    "transient_reward_10_step_2": "1.2500",
    "transient_reward_10_step_3": "1.1250",
    "transient_reward_10_step_4": "1.0625",
    "transient_reward_10_step_5": "1.0312",
    "transient_reward_10_step_6": "1.0156",
    "transient_reward_10_step_7": "1.0078",
    "transient_reward_10_step_8": "1.0039",
    "transient_reward_10_step_9": "1.0020",
    "transient_reward_10_step_10": "1.0010",
    "markovTrace_0": "[A_s0]",
    "markovTrace_15": "[A_s0, A_s0, A_s0, B1, B1, C, C, C, B1, C, B1, B1, C, B1, B1, C]",
    "longRunExpectedAverageReward_cycle": "[[0.6667, 0.6667], 0.0000, 0.0000, 0.6667, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[[None, None], None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[0.5985, 1.1793], 0.2904, 0.4851, 0.8889, 12.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[0.6029, 1.0894], 0.2433, 0.4035, 0.8462, 11.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[[0.0000, 0.5000, 0.5000], [[0.0000, 0.0000], [0.5000, 0.5000], [0.5000, 0.5000]], [0.0000, 0.0000, 0.0000], [None, 0.0000, 0.0000], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[[0.0000, 0.0000, 0.0000], [[None, None], [None, None], [None, None]], [None, None, None], [None, None, None], 0.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[0.0000, 0.5000, 0.5000], [[0.0000, 0.0000], [0.3590, 0.6410], [0.3590, 0.6410]], [0.0000, 0.1410, 0.1410], [None, 0.3929, 0.3929], 13.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[[0.0000, 0.5455, 0.4545], [[0.0000, 0.0000], [0.3998, 0.6911], [0.3089, 0.6002]], [0.0000, 0.1457, 0.1457], [None, 0.3644, 0.4716], 30.0000, Relative Error]",
    "estimationExpectedReward_step": "[1.0000, [1.0000, 1.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[1.5333, [1.3548, 1.7119], 0.1785, 0.1318, 30.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[1.4000, [1.2247, 1.5753], 0.1753, 0.1431, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 1.0000, 0.0000], [[0.0000, 0.0000], [1.0000, 1.0000], [0.0000, 0.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000], [None, 0.0000, None], [1.0000, None, 1.0000], [[None, None], [0.0000, 0.0000], [None, None]], [Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000], [[1.0000, 1.0000], [1.0000, 1.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000], [None, None, 0.0000], [1.0000, 1.0000, None], [[None, None], [None, None], [0.0000, 0.0000]], [Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[0.0000, 0.0000, 1.0000], [1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000], [None, None, 0.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], [Number of Paths, Number of Paths, Number of Paths]]"
}