{
    "states": "[A]",
    "rewardVector": "[1]",
    "executeSteps_0": "[[1]]",
    "executeSteps_15": "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]",
    "classifyTransientRecurrent": "[, [A]]",
    "classifyPeriodicity": {
        "A": "1.0000"
    },
    "determineMCType": "ergodic unichain",
    "hittingProbabilities": {
        "A": "1"
    },
    "limitingMatrix": "[[1]]",
    "limitingDistribution": "[1]",
    "longRunReward": "1",
    "transient_reward_0_step_0": "1",
    "transient_reward_10_step_0": "1",
    "transient_reward_10_step_1": "1",
    "transient_reward_10_step_2": "1",
    "transient_reward_10_step_3": "1",
    "transient_reward_10_step_4": "1",
    "transient_reward_10_step_5": "1",
    "transient_reward_10_step_6": "1",
    "transient_reward_10_step_7": "1",
    "transient_reward_10_step_8": "1",
    "transient_reward_10_step_9": "1",
    "transient_reward_10_step_10": "1",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A]",
    "longRunExpectedAverageReward_cycle": "[None, None, None, None, None, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, None, None, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 0.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 0.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [None], [None], None, None, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None], [None], None, None, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[[1.0000, 1.0000]], [0.0000], [0.0000], [1.0000], [0.0000], Absolute Error]",
    "cezaroLimitDistribution_rel": "[[[1.0000, 1.0000]], [0.0000], [0.0000], [1.0000], [0.0000], Relative Error]",
    "estimationExpectedReward_step": "[None, None, None, None, None, Number of Paths]",
    "estimationExpectedReward_abs": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 0.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 0.0000, Relative Error]",
    "estimationDistribution": "[None, [None], [None], None, None, Number of Paths]",
    "estimationHittingState": "[{'A': '[None, None, None, None, None]'}, {'A': 'Number of Paths'}]",
    "estimationHittingreward": "[{'A': '[None, None, None, None, None]'}, {'A': 'Number of Paths'}]",
    "estimationHittingStateSet": "[{'A': '[None, None, None, None, None]'}, {'A': 'Number of Paths'}]",
    "estimationHittingRewardSet": "[{'A': '[None, None, None, None, None]'}, {'A': 'Number of Paths'}]",
    "convert_to_DSL": "markov chain TestName {\n\tA [p: 1; r: 1] -- 1 --> A\n}\n"
}