{
    "states": "[A]",
    "rewardVector": "[1]",
    "executeSteps_0": "[[1]]",
    "executeSteps_15": "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]",
    "classifyTransientRecurrent": "[, [A]]",
    "classifyPeriodicity": {
        "A": "1.0000"
    },
    "determineMCType": "ergodic unichain",
    "hittingProbabilities": {
        "A": "1"
    },
    "limitingMatrix": "[[1]]",
    "limitingDistribution": "[1]",
    "longRunReward": "1",
    "transient_reward_0_step_0": "1",
    "transient_reward_10_step_0": "1",
    "transient_reward_10_step_1": "1",
    "transient_reward_10_step_2": "1",
    "transient_reward_10_step_3": "1",
    "transient_reward_10_step_4": "1",
    "transient_reward_10_step_5": "1",
    "transient_reward_10_step_6": "1",
    "transient_reward_10_step_7": "1",
    "transient_reward_10_step_8": "1",
    "transient_reward_10_step_9": "1",
    "transient_reward_10_step_10": "1",
    "markovTrace_0": "[A]",
    "markovTrace_15": "[A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A]",
    "longRunExpectedAverageReward_cycle": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 1.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 30.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[1.0000, 1.0000], 0.0000, 0.0000, 1.0000, 30.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [[-1.9600, 1.9600]], [1.9600], [None], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [[-1.9600, 1.9600]], [1.9600], [None], 1.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[None, [[-0.3578, 0.3578]], [0.3578], [None], 30.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[None, [[-0.0620, 0.0620]], [0.0620], [None], 1000.0000, Maximum path length]",
    "estimationExpectedReward_step": "[1.0000, [1.0000, 1.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[1.0000, [1.0000, 1.0000], 0.0000, 0.0000, 30.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[1.0000, [1.0000, 1.0000], 0.0000, 0.0000, 30.0000, Relative Error]",
    "estimationDistribution": "[[1.0000], [[1.0000, 1.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided], [1.0000], [0.0000], [None], [[0.0000, 0.0000]], [Number of Paths]]",
    "estimationHittingreward": "[[1.0000], [1.0000], [0.0000], [0.0000], [[1.0000, 1.0000]], [Number of Paths]]",
    "estimationHittingStateSet": "[[1.0000], [1.0000], [0.0000], [0.0000], [[1.0000, 1.0000]], [Number of Paths]]",
    "estimationHittingRewardSet": "[[1.0000], [1.0000], [0.0000], [0.0000], [[1.0000, 1.0000]], [Number of Paths]]"
}