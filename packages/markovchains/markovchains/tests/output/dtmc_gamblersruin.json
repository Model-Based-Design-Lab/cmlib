{
    "states": "[S1, S2, S3, S4]",
    "rewardVector": "[0, 100, 200, 300]",
    "executeSteps_0": "[[0, 1/2, 1/2, 0]]",
    "executeSteps_15": "[[0, 1/2, 1/2, 0], [1/4, 1/4, 1/4, 1/4], [3/8, 1/8, 1/8, 3/8], [7/16, 1/16, 1/16, 7/16], [15/32, 1/32, 1/32, 15/32], [31/64, 1/64, 1/64, 31/64], [63/128, 1/128, 1/128, 63/128], [127/256, 1/256, 1/256, 127/256], [255/512, 1/512, 1/512, 255/512], [511/1024, 1/1024, 1/1024, 511/1024], [1023/2048, 1/2048, 1/2048, 1023/2048], [2047/4096, 1/4096, 1/4096, 2047/4096], [4095/8192, 1/8192, 1/8192, 4095/8192], [8191/16384, 1/16384, 1/16384, 8191/16384], [16383/32768, 1/32768, 1/32768, 16383/32768], [32767/65536, 1/65536, 1/65536, 32767/65536]]",
    "classifyTransientRecurrent": "[[S2, S3], [S1, S4]]",
    "classifyPeriodicity": {
        "S1": "1.0000",
        "S4": "1.0000"
    },
    "determineMCType": "ergodic non-unichain",
    "hittingProbabilities": {
        "S1": "1",
        "S2": "2/3",
        "S3": "1/3",
        "S4": "0"
    },
    "limitingMatrix": "[[1, 2/3, 1/3, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1/3, 2/3, 1]]",
    "limitingDistribution": "[1/2, 0, 0, 1/2]",
    "longRunReward": "150",
    "transient_reward_0_step_0": "150",
    "transient_reward_10_step_0": "150",
    "transient_reward_10_step_1": "150",
    "transient_reward_10_step_2": "150",
    "transient_reward_10_step_3": "150",
    "transient_reward_10_step_4": "150",
    "transient_reward_10_step_5": "150",
    "transient_reward_10_step_6": "150",
    "transient_reward_10_step_7": "150",
    "transient_reward_10_step_8": "150",
    "transient_reward_10_step_9": "150",
    "transient_reward_10_step_10": "150",
    "markovTrace_0": "[S3]",
    "markovTrace_15": "[S2, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1]",
    "longRunExpectedAverageReward_cycle": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 29.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 29.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [-1.9600, 1.9600]], [0.0000, 0.0000, 0.0000, 1.9600], [None, None, None, None], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], [None, None, None, None], -1.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[None, [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [-0.3640, 0.3640]], [0.0000, 0.0000, 0.0000, 0.3640], [None, None, None, None], 29.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[None, [[-0.0620, 0.0620], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [0.0620, 0.0000, 0.0000, 0.0000], [None, None, None, None], 999.0000, Maximum path length]",
    "estimationExpectedReward_step": "[300.0000, [300.0000, 300.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[149.9285, [149.4285, 150.4285], 0.5000, 0.0033, 192215.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[143.3333, [102.3421, 184.3246], 40.9912, 0.4005, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 0.0000, 0.0000, 1.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, None], [None, 1.0000, 1.0000, 1.0000], [[0.0000, 0.0000], None, None, None], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 0.0000, 0.0000, 0.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, None], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, None], [None, 1.0000, 1.0000, 1.0000], [[0.0000, 0.0000], None, None, None], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[1.0000, 0.0000, 0.0000, 0.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, None], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]"
}