{
    "states": "[S1, S2, S3, S4]",
    "rewardVector": "[0, 100, 200, 300]",
    "executeSteps_0": "[[0, 1/2, 1/2, 0]]",
    "executeSteps_15": "[[0, 1/2, 1/2, 0], [1/4, 1/4, 1/4, 1/4], [3/8, 1/8, 1/8, 3/8], [7/16, 1/16, 1/16, 7/16], [15/32, 1/32, 1/32, 15/32], [31/64, 1/64, 1/64, 31/64], [63/128, 1/128, 1/128, 63/128], [127/256, 1/256, 1/256, 127/256], [255/512, 1/512, 1/512, 255/512], [511/1024, 1/1024, 1/1024, 511/1024], [1023/2048, 1/2048, 1/2048, 1023/2048], [2047/4096, 1/4096, 1/4096, 2047/4096], [4095/8192, 1/8192, 1/8192, 4095/8192], [8191/16384, 1/16384, 1/16384, 8191/16384], [16383/32768, 1/32768, 1/32768, 16383/32768], [32767/65536, 1/65536, 1/65536, 32767/65536]]",
    "classifyTransientRecurrent": "[[S2, S3], [S1, S4]]",
    "classifyPeriodicity": {
        "S1": "1.0000",
        "S4": "1.0000"
    },
    "determineMCType": "ergodic non-unichain",
    "hittingProbabilities": {
        "S1": "1",
        "S2": "2/3",
        "S3": "1/3",
        "S4": "0"
    },
    "limitingMatrix": "[[1, 2/3, 1/3, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1/3, 2/3, 1]]",
    "limitingDistribution": "[1/2, 0, 0, 1/2]",
    "longRunReward": "150",
    "transient_reward_0_step_0": "150",
    "transient_reward_10_step_0": "150",
    "transient_reward_10_step_1": "150",
    "transient_reward_10_step_2": "150",
    "transient_reward_10_step_3": "150",
    "transient_reward_10_step_4": "150",
    "transient_reward_10_step_5": "150",
    "transient_reward_10_step_6": "150",
    "transient_reward_10_step_7": "150",
    "transient_reward_10_step_8": "150",
    "transient_reward_10_step_9": "150",
    "transient_reward_10_step_10": "150",
    "markovTrace_0": "[S3]",
    "markovTrace_15": "[S2, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1]",
    "longRunExpectedAverageReward_cycle": "[None, None, None, None, None, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, None, None, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[299.2449, 300.2436], 0.4993, 0.0017, 299.7442, 5.0443, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[-0.0956, 0.2954], 0.1955, None, 0.0999, 3.1575, Maximum path length]",
    "cezaroLimitDistribution_cycle": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], None, None, Maximum path length]",
    "cezaroLimitDistribution_abs": "[[[0.7450, 0.9883], [-0.0226, 0.1559], [-0.0226, 0.1559], [0.0000, 0.0000]], [0.1216, 0.0893, 0.0893, 0.0000], [0.1633, None, None, None], [0.8667, 0.0667, 0.0667, 0.0000], [0.3399, 0.2494, 0.2494, 0.0000], Absolute Error]",
    "cezaroLimitDistribution_rel": "[[[0.0000, 0.0000], [-0.0010, 0.0030], [-0.0010, 0.0030], [0.9952, 1.0008]], [0.0000, 0.0020, 0.0020, 0.0028], [None, None, None, 0.0028], [0.0000, 0.0010, 0.0010, 0.9980], [0.0000, 0.0316, 0.0316, 0.0447], Maximum path length]",
    "estimationExpectedReward_step": "[None, None, None, None, None, Number of Paths]",
    "estimationExpectedReward_abs": "[[149.3244, 150.3244], 0.5000, 0.0033, 149.8244, 111.7706, Absolute Error]",
    "estimationExpectedReward_rel": "[[128.1711, 198.4955], 35.1622, 0.2743, 163.3333, 98.2627, Relative Error]",
    "estimationDistribution": "[None, [None, None, None, None], [None, None, None, None], None, None, Number of Paths]",
    "estimationHittingState": "[{'S1': '[None, None, None, None, None]', 'S2': '[None, None, None, None, None]', 'S3': '[None, None, None, None, None]', 'S4': '[None, None, None, None, None]'}, {'S1': 'Number of Paths', 'S2': 'Number of Paths', 'S3': 'Number of Paths', 'S4': 'Number of Paths'}]",
    "estimationHittingreward": "None",
    "estimationHittingStateSet": "[{'S1': '[None, None, None, None, None]', 'S2': '[None, None, None, None, None]', 'S3': '[None, None, None, None, None]', 'S4': '[None, None, None, None, None]'}, {'S1': 'Number of Paths', 'S2': 'Number of Paths', 'S3': 'Number of Paths', 'S4': 'Number of Paths'}]",
    "estimationHittingRewardSet": "None"
}