{
    "states": "[S1, S2, S3, S4]",
    "rewardVector": "[0.0000, 100.0000, 200.0000, 300.0000]",
    "executeSteps_0": "[[0.0000, 0.5000, 0.5000, 0.0000]]",
    "executeSteps_15": "[[0.0000, 0.5000, 0.5000, 0.0000], [0.2500, 0.2500, 0.2500, 0.2500], [0.3750, 0.1250, 0.1250, 0.3750], [0.4375, 0.0625, 0.0625, 0.4375], [0.4688, 0.0312, 0.0312, 0.4688], [0.4844, 0.0156, 0.0156, 0.4844], [0.4922, 0.0078, 0.0078, 0.4922], [0.4961, 0.0039, 0.0039, 0.4961], [0.4980, 0.0020, 0.0020, 0.4980], [0.4990, 0.0010, 0.0010, 0.4990], [0.4995, 0.0005, 0.0005, 0.4995], [0.4998, 0.0002, 0.0002, 0.4998], [0.4999, 0.0001, 0.0001, 0.4999], [0.4999, 0.0001, 0.0001, 0.4999], [0.5000, 0.0000, 0.0000, 0.5000], [0.5000, 0.0000, 0.0000, 0.5000]]",
    "classifyTransientRecurrent": "[[S2, S3], [S1, S4]]",
    "classifyPeriodicity": {
        "S1": "1.0000",
        "S4": "1.0000"
    },
    "determineMCType": "ergodic non-unichain",
    "hittingProbabilities": {
        "S1": "1.0000",
        "S2": "0.6667",
        "S3": "0.3333",
        "S4": "0.0000"
    },
    "limitingMatrix": "[[1.0000, 0.0000, 0.0000, 0.0000], [0.6667, 0.0000, 0.0000, 0.3333], [0.3333, 0.0000, 0.0000, 0.6667], [0.0000, 0.0000, 0.0000, 1.0000]]",
    "limitingDistribution": "[0.5000, 0.0000, 0.0000, 0.5000]",
    "longRunReward": "150.0000",
    "transient_reward_0_step_0": "150.0000",
    "transient_reward_10_step_0": "150.0000",
    "transient_reward_10_step_1": "150.0000",
    "transient_reward_10_step_2": "150.0000",
    "transient_reward_10_step_3": "150.0000",
    "transient_reward_10_step_4": "150.0000",
    "transient_reward_10_step_5": "150.0000",
    "transient_reward_10_step_6": "150.0000",
    "transient_reward_10_step_7": "150.0000",
    "transient_reward_10_step_8": "150.0000",
    "transient_reward_10_step_9": "150.0000",
    "transient_reward_10_step_10": "150.0000",
    "markovTrace_0": "[S3]",
    "markovTrace_15": "[S2, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1]",
    "longRunExpectedAverageReward_cycle": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 1.0000, Number of cycles]",
    "longRunExpectedAverageReward_steps": "[None, None, None, 0.0000, 0.0000, Maximum path length]",
    "longRunExpectedAverageReward_abs": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 29.0000, Absolute Error]",
    "longRunExpectedAverageReward_rel": "[[300.0000, 300.0000], 0.0000, 0.0000, 300.0000, 29.0000, Relative Error]",
    "cezaroLimitDistribution_cycle": "[None, [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [-1.9600, 1.9600]], [0.0000, 0.0000, 0.0000, 1.9600], [None, None, None, None], 1.0000, Number of cycles]",
    "cezaroLimitDistribution_steps": "[None, [None, None, None, None], [None, None, None, None], [None, None, None, None], -1.0000, Maximum path length]",
    "cezaroLimitDistribution_abs": "[None, [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [-0.3640, 0.3640]], [0.0000, 0.0000, 0.0000, 0.3640], [None, None, None, None], 29.0000, Absolute Error]",
    "cezaroLimitDistribution_rel": "[None, [[-0.0620, 0.0620], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [0.0620, 0.0000, 0.0000, 0.0000], [None, None, None, None], 999.0000, Maximum path length]",
    "estimationExpectedReward_step": "[300.0000, [300.0000, 300.0000], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationExpectedReward_abs": "[149.9285, [149.4285, 150.4285], 0.5000, 0.0033, 192215.0000, Absolute Error]",
    "estimationExpectedReward_rel": "[143.3333, [102.3421, 184.3246], 40.9912, 0.4005, 30.0000, Relative Error]",
    "estimationDistribution": "[[0.0000, 0.0000, 0.0000, 1.0000], [[0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000], [1.0000, 1.0000]], 0.0000, 0.0000, 1.0000, Number of Paths]",
    "estimationHittingState": "[[Cannot be decided, Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, None], [None, 1.0000, 1.0000, 1.0000], [[0.0000, 0.0000], None, None, None], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingreward": "[[1.0000, 0.0000, 0.0000, 0.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, None], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingStateSet": "[[Cannot be decided, Cannot be decided, Cannot be decided, Cannot be decided], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, None, None, None], [None, 1.0000, 1.0000, 1.0000], [[0.0000, 0.0000], None, None, None], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]",
    "estimationHittingRewardSet": "[[1.0000, 0.0000, 0.0000, 0.0000], [1.0000, 1.0000, 1.0000, 1.0000], [0.0000, 0.0000, 0.0000, 0.0000], [0.0000, None, None, None], [[1.0000, 1.0000], [0.0000, 0.0000], [0.0000, 0.0000], [0.0000, 0.0000]], [Number of Paths, Number of Paths, Number of Paths, Number of Paths]]"
}